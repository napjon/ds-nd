{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: \n",
    "\n",
    "* Based on the email and financial dataset, predict whether Enron employee is fraud.\n",
    "* Use two evaluation metrics, precision and recall above 0.3\n",
    "\n",
    "Machine Learning\n",
    "\n",
    "* Use Naive Bayes for person classification, \n",
    "* preprocess with tools/email_preprocess. word_data contain list of words, email_authors contain the authors in numeric. Possibily we extract the name and the email, put it to preprocess.\n",
    "* Use SVM, using 1% features can generate 88-99% accuracy.\n",
    "* Naive Bayes is better at classifying text than SVM. Faster and gives better performance.\n",
    "* Decision Tree:\n",
    "    * Min sample split: Minimum sample to split. >> to avoid overfitting\n",
    "    * Feature selection based on information gain.\n",
    "    * SelectPercentile to limit features, more features, more complex fit.\n",
    "* Use RandomForest has built-in overfitting(Randomly samples, bootstrap, majority votes(ensemble))\n",
    "* Use feature_format to convert dictionary into list of features2\n",
    "\n",
    "        feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "        data_array = featureFormat( data_dictionary, feature_list )\n",
    "        label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "        the line above (targetFeatureSplit) assumes that the\n",
    "        label is the _first_ item in feature_list--very important\n",
    "        that poi is listed first!\n",
    "        \n",
    "* LinearRegression, long_term_incentive will do more to bonus than saalary\n",
    "* Outlier can greatly affected slope in linear regression\n",
    "* Visualize online outliers in outlier/enron_outliers.py\n",
    "* After remove outliers, Salary and Bonus is important numerical feature detecting POI!!!\n",
    "\n",
    "* Not using clustering since we already have labeled data.\n",
    "\n",
    "* Feature scaling not in SVM or kMeans, they both scale the features that dependent. Linear Regression and Decision tree are those models that have independent variable. (DT: split on 1 var, LR: coef every variable)\n",
    "\n",
    "* SelectPercentile: percent best features. SelectKBest: based on K\n",
    "* Use TfIdfVectorizer in words that frequent need to be filtered.\n",
    "* LasoRegression to avoid overfitting (regularization)\n",
    "* call `clf.feature_importances_` to see which features drive the most performance.\n",
    "* Use vectorizer and the method above to find what words that are signature of POI-s\n",
    "* Use stemmer (SnowballStemmer) to stem words.\n",
    "* Use ../tools/parse_out_email_text.py to parse out text from email.\n",
    "* Use text_learning/vectorize_text.py to parse out stemmed words, label 1 if from poi, 0 otherwise\n",
    "* use PCA to find latent features.\n",
    "* F! score best of both world\n",
    "* Use train_test_split, GridSearchCV to get the best parameter.\n",
    "\n",
    "Methods:\n",
    "\n",
    "* Use poi_names for labelling, connect it to enron dataset dictionary\n",
    "* The dictionary has email, if we want to see from-to enron email, crawl that dataset.\n",
    "* the dataset has the form\n",
    "    enron_data[\"LASTNAME FIRSTNAME MIDDLEINITIAL\"] = { features_dict }\n",
    "* Knowing best features with human understanding about the dataset.\n",
    "* No training points would have \"NaN\" for total_payments when the class label is \"POI\"\n",
    "* Now there are 156 folks in dataset, 31 of whom have \"NaN\" total_payments. This makes for 20% of them with a \"NaN\" overall.\n",
    "* Added new number from dataset, there are 36% of POI have NaN for total payment\n",
    "* Don't try to remove that outliers that we want to pay attention\n",
    "* Remove outliers by removing 10% highest residual erros, and try again (outliers/outlier_cleaner.py)\n",
    "* Remove ['TOTAL'] outlier (it's not a person) in the final_project_dataset.\n",
    "* Two out of 4 ourliers next visualization(after remove total, outlier/enron_outliers.py) is poi indentified. Not sure about the rest, check again whether or it's mistypo.\n",
    "* feature scale all of your numerical features,  including salary and exercised stock options.\n",
    "* Critical to feature scale n-message and salary! first one can be counted, later will have the order of thousands.\n",
    "\n",
    "* Intuition: POI sends another POI higher email rate than general\n",
    "* visualize: what features that makes POI stands out than general?\n",
    "* Use feature_selection/poi_flag_email.py to identify whether poi, based on given email file\n",
    "* Use feature_selection/new_enron_feature.py to create new feature, *count emails from poi*\n",
    "* Use feature_selection/visualize_new_feature.py to create fraction of from_poi/from_general ratio\n",
    "\n",
    "\n",
    "Problems\n",
    "\n",
    "* POI could end up not being in the dataset (Michael Krautz)\n",
    "* Inconsistent naming format in text emails.\n",
    "* Total payments could be the feature. NaN for non POI. Use isTotalPaymentExist for NaN categorical. (THIS INFORMATION CAN BE PICKED BY THE MODEL TO IDENTIFY POI). This is wrong, and as we added new dataset, this feature is normalized. \n",
    "* DIfferent data sources can introduce difference bias and mistake. \n",
    "* Financial features have some bias, you have to handle it. It doesn't if you just use email data.\n",
    "* Feature scale salary and stock might be an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=1e-06, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=31, n_jobs=-1,\n",
      "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "\tAccuracy: 0.79058\tPrecision: 0.36853\tRecall: 0.35950\tF1: 0.36396\tF2: 0.36127\n",
      "\tTotal predictions: 12000\tTrue positives:  719\tFalse positives: 1232\tFalse negatives: 1281\tTrue negatives: 8768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
      "       warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'loss': ['hinge', 'log', 'squared_hinge'], 'n_iter': [30, 35], 'alpha': [0.01, 0.0001, 1e-06]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)\n",
      "\tAccuracy: 0.78658\tPrecision: 0.35534\tRecall: 0.34450\tF1: 0.34983\tF2: 0.34661\n",
      "\tTotal predictions: 12000\tTrue positives:  689\tFalse positives: 1250\tFalse negatives: 1311\tTrue negatives: 8750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
      "       warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'loss': ['hinge', 'log', 'squared_hinge'], 'n_iter': [30, 35], 'alpha': [0.01, 0.0001, 1e-06]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)\n",
      "\tAccuracy: 0.78225\tPrecision: 0.34924\tRecall: 0.35500\tF1: 0.35210\tF2: 0.35383\n",
      "\tTotal predictions: 12000\tTrue positives:  710\tFalse positives: 1323\tFalse negatives: 1290\tTrue negatives: 8677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deferred_income',\n",
       " 'bonus',\n",
       " 'total_stock_value',\n",
       " 'salary',\n",
       " 'exercised_stock_options']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_percentiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = pickle.load(open('my_classifier.pkl','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting poi_id.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile,f_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "sys.path.append(\"../tools\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from visualize_new_feature import update_with_fraction_poi\n",
    "from vectorize_text import get_word_data_from_email\n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open('final_project_dataset.pkl', \"r\") )\n",
    "\n",
    "## Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL')\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "data_with_fraction_poi = update_with_fraction_poi(data_dict)#Update dataset with fraction poi\n",
    "\n",
    "\n",
    "full_df = pd.DataFrame.from_dict(data_with_fraction_poi,orient='index') #create pandas Dataframe from dataset\n",
    "df = full_df[full_df.email_address != 'NaN']\n",
    "\n",
    "cols = df.columns.tolist() # get the list of features\n",
    "cols.remove('email_address')#remove non numeric features\n",
    "cols.remove('poi')# remove labels\n",
    "impute = df[cols].copy().applymap(lambda x: 0  if x == 'NaN' else x) #replace NaN features as 0\n",
    "\n",
    "scaled = impute.apply(MinMaxScaler().fit_transform) #Scaled each of the feature \n",
    "\n",
    "selPerc = SelectPercentile(f_classif,percentile=21) # Built the SelectPercentile, 21 Selected Based on the Performance\n",
    "selPerc.fit(scaled,df['poi']) # Learn the Features, knowing which features to use\n",
    "\n",
    "features_percentiled = scaled.columns[selPerc.get_support()].tolist() #Filter columns based on what Percentile support\n",
    "scaled['poi'] = df['poi'] #rejoin the label\n",
    "\n",
    "###### Add Text Learning####\n",
    "word_data = df['email_address'].apply(get_word_data_from_email) # Extract words given email\n",
    "vect = TfidfVectorizer(stop_words='english',max_df=0.4,min_df=0.33) # Build the vectorizer\n",
    "vect.fit(word_data) # Vectorizer Learn from data\n",
    "words = vect.vocabulary_.keys() # what words to used\n",
    "vectorized_words = vect.transform(word_data).toarray()# The values of vectorized words\n",
    "df_docs = pd.DataFrame(vectorized_words, \n",
    "                       columns=words,\n",
    "                       index=df.index) # Using same index, person\n",
    "df_with_data = pd.concat([df_docs,scaled],axis=1) # Concat emails with numerical features\n",
    "############################\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = df_with_data.to_dict(orient='index') #change the dataframe back to dictionary\n",
    "# my_dataset = scaled.to_dict(orient='index') #change the dataframe back to dictionary\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi'] + features_percentiled  + words# You will need to use more features\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB ##Default(Tuned): Precision: 0.29453\tRecall: 0.43650\n",
    "from sklearn.tree import DecisionTreeClassifier ##Default: Precision: 0.14830\tRecall: 0.05450\n",
    "from sklearn.ensemble import RandomForestClassifier ##Default: Precision: 0.47575\tRecall: 0.20600, Longer time\n",
    "from sklearn.linear_model import SGDClassifier ##Tuned: Precision: 0.36853\tRecall: 0.35950, BEST!\n",
    "\n",
    "\n",
    "\n",
    "# clf = SGDClassifier(loss='hinge',\n",
    "#                     penalty='l2',\n",
    "#                     alpha=1e-6,\n",
    "#                     n_iter=31,\n",
    "#                     n_jobs=-1,\n",
    "#                     random_state=42)\n",
    "\n",
    "# clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "folds = 1000\n",
    "\n",
    "# StratifiedShuffleSplit is used when we take advantage of skew data but still keeping proportion of labels\n",
    "# If we using usual train test split, it could be there's no POI labels in the test set, or even worse in train set\n",
    "# which would makes the model isn't good enough. If for example the StratifiedShuffleSplit have 10 folds, then every folds\n",
    "# will contains equal proportions of POI vs non-POI\n",
    "\n",
    "cv = StratifiedShuffleSplit(df.poi,10,random_state=42)\n",
    "\n",
    "sgd = SGDClassifier(penalty='l2',random_state=42)\n",
    "parameters = {'loss': ['hinge','log','squared_hinge'],\n",
    "              'n_iter': [30,35],\n",
    "              'alpha': [1e-2, 1e-4 ,1e-6],\n",
    "              }\n",
    "\n",
    "clf = GridSearchCV(sgd,parameters,scoring='f1',cv=10)\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize email py\n",
    "\n",
    "Given an email, parse the text from the email\n",
    "\n",
    "Look up vectorize_text.py\n",
    "\n",
    "if the email in poi_email_address.py/poiEmails(), then from_poi append 1 otherwise 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.12080033,  0.521875  , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.99854354,  0.        , ...,  0.        ,\n",
       "         0.03904287,  0.        ],\n",
       "       [ 0.        ,  0.94246039,  0.05      , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  1.        ,  0.05625   , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.        ,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load ../tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vectorize_text import get_word_data_from_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict  = pickle.load(open('final_project_dataset.pkl','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['word_data'] = df['email_address'].apply(get_word_data_from_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english',max_df=0.8,min_df=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<145x518 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 13491 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit_transform(df['word_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_docs = pd.DataFrame(vect.fit_transform(df['word_data']).toarray(),\n",
    "                       columns=vect.vocabulary_.keys(),\n",
    "                       index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussianNB().fit(vect.fit_transform(df['word_data']).toarray(), df.poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'BADUM JAMES P', u'BAXTER JOHN C', u'BAZELIDES PHILIP J',\n",
       "       u'BELFER ROBERT', u'BLAKE JR. NORMAN P', u'CHAN RONNIE',\n",
       "       u'CLINE KENNETH W', u'CUMBERLAND MICHAEL S', u'DUNCAN JOHN H',\n",
       "       u'FUGH JOHN L', u'GAHN ROBERT S', u'GATHMANN WILLIAM D', u'GILLIS JOHN',\n",
       "       u'GRAMM WENDY L', u'GRAY RODNEY', u'JAEDICKE ROBERT',\n",
       "       u'LEMAISTRE CHARLES', u'LOCKHART EUGENE E', u'LOWRY CHARLES P',\n",
       "       u'MENDELSOHN JOHN', u'MEYER JEROME J', u'NOLES JAMES L',\n",
       "       u'PEREIRA PAULO V. FERRAZ', u'REYNOLDS LAWRENCE', u'SAVAGE FRANK',\n",
       "       u'SULLIVAN-SHAKLOVITZ COLLEEN', u'THE TRAVEL AGENCY IN THE PARK',\n",
       "       u'URQUHART JOHN A', u'WAKEHAM JOHN', u'WALTERS GARETH W',\n",
       "       u'WHALEY DAVID A', u'WINOKUR JR. HERBERT S', u'WROBEL BRUCE',\n",
       "       u'YEAP SOON'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.email_address == 'NaN'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_modified = pd.DataFrame.from_dict(pickle.load(open('../dataset/final_project_dataset_modified.pkl','r')),orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
